We address the problem of automatically constructing a thesaurus by clustering words based on corpus data .
We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation .
We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter .
We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus .
Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation .
Recently various methods for automatically constructing a thesaurus ( hierarchically clustering words ) based on corpus data have been proposed Hindle 1990 , Brown et al. 1992 , Pereira et al. 1993, Tokunaga et al. 1995 .
In this paper , we propose a new method for automatic construction of thesauri .
Specifically , we view the problem of automatically clustering words as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns ( in general , any set of words ) and a partition of a set of verbs ( in general , any set of words ) , and propose an estimation algorithm using simulated annealing with an energy function based on the Minimum Description Length ( MDL ) Principle .
The MDL Principle is a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics .
As a strategy of statistical estimation MDL is guaranteed to be near optimal .
We empirically evaluated the effectiveness of our method .
In particular , we compared the performance of an MDL-based simulated annealing algorithm in hierarchical word clustering against that of one based on the Maximum Likelihood Estimator ( MLE , for short ) .
We found that the MDL-based method performs better than the MLE-based method .
We also evaluated our method by conducting pp-attachment disambiguation experiments using a thesaurus automatically constructed by it and found that disambiguation results can be improved .
Since some words never occur in a corpus , and thus cannot be reliably classified by a method solely based on corpus data , we propose to combine the use of an automatically constructed thesaurus and a hand made thesaurus in disambiguation .
We conducted some experiments in order to test the effectiveness of this strategy .
Our experimental results indicate that combining an automatically constructed thesaurus and a hand made thesaurus widens the coverage of our disambiguation method , while maintaining high accuracy .
A method of constructing a thesaurus based on corpus data usually consists of the following three steps :
Extract co-occurrence data ( e.g. case frame data , adjacency data ) from a corpus ,
Starting from a single class ( or each word composing its own class ) , divide ( or merge ) word classes based on the co-occurrence data using some similarity ( distance ) measure .
( The former approach is called ` divisive , ' the latter ` agglomerative ' )
Repeat step  until some stopping condition is met , to construct a thesaurus ( tree ) .
The method we propose here consists of the same three steps .
Suppose available to us are data like those in Figure  , which are frequency data ( co-occurrence data ) between verbs and their objects extracted from a corpus ( step  ) .
We then view the problem of clustering words as that of estimating a probabilistic model ( representing probability distribution ) that generates such data .
We assume that the target model can be defined in the following way .
First , we define a noun partition  over a given set of nouns  and a verb partion  over a given set of verbs  .
A noun partition is any set  satisfying  ,  and  .
A verb partition  is defined analogously .
In this paper , we call a member of a noun partition a ` noun cluster , ' and a member of a verb partition a ` verb cluster ' .
We refer to a member of the Cartesian product of a noun partition and a verb partition (  ) simply as a ` cluster ' .
We then define a probabilistic model ( a joint distribution ) , written  , where random variable  assumes a value from a fixed noun partition  , and  a value from a fixed verb partition  .
Within a given cluster , we assume that each element is generated with equal probability , i.e. ,
Figure  shows two example models which might have given rise to the data in Figure  .
In this paper , we assume that the observed data are generated by a model belonging to the class of models just described , and select a model which best explains the data .
As a result of this , we obtain both noun clusters and verb clusters .
This problem setting is based on the intuitive assumption that similar words occur in the same context with roughly equal likelihood , as is made explicit in equation  .
Thus selecting a model which best explains the given data is equivalent to finding the most appropriate classification of words based on their co-occurrence .
We now turn to the question of what strategy ( or criterion ) we should employ for estimating the best model .
Our choice is the MDL ( Minimum Description Length ) principle Rissanen 1978 , Rissanen 1983 , Rissanen 1984 , Rissanen 1986 , Rissanen 1989 , a well-known principle of data compression and statistical estimation from information theory .
MDL stipulates that the best probability model for given data is that model which requires the least code length for encoding of the model itself , as well as the given data relative to it .
We refer to the code length for the model as the ` model description length ' and that for the data the ` data description length ' .
We apply MDL to the problem of estimating a model consisting of a pair of partitions as described above .
In this context , a model with less clusters , such as Model 2 in Figure  , tends to be simpler ( in terms of the number of parameters ) , but also tends to have a poorer fit to the data .
In contrast , a model with more clusters , such as Model 1 in Figure  , is more complex , but tends to have a better fit to the data .
Thus , there is a trade-off relationship between the simplicity of clustering ( a model ) and the goodness of fit to the data .
The model description length quantifies the simplicity ( complexity ) of a model , and the data description length quantifies the fit to the data .
According to MDL , the model which minimizes the sum total of the two types of description lengths should be selected .
In what follows , we will describe in detail how the description length is to be calculated in our current context , as well as our simulated annealing algorithm based on MDL .
We will now describe how the description length for a model is calculated .
Recall that each model is specified by the Cartesian product of a noun partition and a verb partition , and a number of parameters for them .
Here we let  denote the size of the noun partition , and  the size of the verb partition .
Then , there are  free parameters in a model .
Given a model M and data S , its total description length L ( M ) is computed as the sum of the model description length  , the description length of its parameters  , and the data description length  .
( We often refer to  as the model description length ) .
Namely ,
We employ the ` binary noun clustering method , ' in which  is fixed at  and we are to decide whether  or  , which is then to be applied recursively to the clusters thus obtained .
This is as if we view the nouns as entities and the verbs as features and cluster the entities based on their features .
Since there are  subsets of the set of nouns  , and for each ` binary ' noun partition we have two different subsets ( a special case of which is when one subset is  and the other the empty set  ) , the number of possible binary noun partitions is  .
Thus for each binary noun partition we need  bits .
Hence  is calculated as
 is calculated by
where | S | denotes the input data size , and  is the number of ( free ) parameters in the model .
It is known that using   bits to describe each of the parameters will ( approximately ) minimize the description length Rissanen 1984 .
Finally ,  is calculated by
where  denotes the observed frequency of the noun verb pair  , and  the estimated probability of  , which is calculated as follows
where  denotes the observed frequency of the noun verb pairs belonging to cluster  .
With the description length of a model defined in the above manner , we wish to select a model having the minimum description length and output it as the result of clustering .
Since the model description length  is the same for each model , in practice we only need to calculate and compare  .
The description lengths for the data in Figure  using the two models in Figure  are shown in Table  .
( Table  shows some values needed for the calculation of the description length for Model 1 . ) These calculations indicate that according to MDL , Model 1 should be selected over Model 2 .
We could in principle calculate the description length for each model and select a model with the minimum description length , if computation time were of no concern .
However , since the number of probabilistic models under consideration is exponential , this is not feasible in practice .
We employ the ` simulated annealing technique ' to deal with this problem .
Figure  shows our ( divisive ) clustering algorithm .
Although there have been many methods of word clustering proposed to date , their objectives seem to vary .
In Table  we exhibit a simple comparison between our work and related work .
Perhaps the method proposed by Pereira et al. 1993 is the most relevant in our context .
In Pereira et al. 1993 , they proposed a method of ` soft clustering , ' namely , each word can belong to a number of distinct classes with certain probabilities .
Soft clustering has several desirable properties .
For example , word sense ambiguities in input data can be treated in a unified manner .
Here , we restrict our attention on ` hard clustering ' ( i.e. , each word must belong to exactly one class ) , in part because we are interested in comparing the thesauri constructed by our method with existing hand-made thesauri .
( Note that a hand made thesaurus is based on hard clustering . )
In this section , we elaborate on the merits of our method .
In statistical natural language processing , usually the number of parameters in a probabilistic model to be estimated is very large , and therefore such a model is difficult to estimate with a reasonable data size that is available in practice .
( This problem is usually referred to as the ` data sparseness problem ' . )
We could smooth the estimated probabilities using an existing smoothing technique Dagan et al. 1992 , Gale and Church 1990 , then calculate some similarity measure using the smoothed probabilities , and then cluster words according to it .
There is no guarantee , however , that the employed smoothing method is in any way consistent with the clustering method used subsequently .
Our method based on MDL resolves this issue in a unified fashion .
By employing models that embody the assumption that words belonging to a same cluster occur in the same context with equal likelihood , our method achieves the smoothing effect as a side effect of the clustering process , where the domains of smoothing coincide with the clusters obtained by clustering .
Thus , the coarseness or fineness of clustering also determines the degree of smoothing .
All of these effects fall out naturally as a corollary of the imperative of ` best possible estimation , ' the original motivation behind the MDL principle .
In our simulated annealing algorithm , we could alternatively employ the Maximum Likelihood Estimator ( MLE ) as criterion for the best probabilistic model , instead of MDL .
MLE , as its name suggests , selects a model which maximizes the likelihood of the data , that is ,  .
This is equivalent to minimizing the ` data description length ' as defined in Section 3 , i.e.  .
We can see easily that MDL generalizes MLE , in that it also takes into account the complexity of the model itself .
In the presence of models with varying complexity , MLE tends to overfit the data , and output a model that is too complex and tailored to fit the specifics of the input data .
If we employ MLE as criterion in our simulated annealing algorithm , it will result in selecting a very fine model with many small clusters , most of which will have probabilities estimated as zero .
Thus , in contrast to employing MDL , it will not have the effect of smoothing at all .
Purely as a method of estimation as well , the superiority of MDL over MLE is supported by convincing theoretical findings Barron and Cover 1991 , Yamanishi 1992 .
For instance , the speed of convergence of the models selected by MDL to the true model is known to be near optimal .
( The models selected by MDL converge to the true model approximately at the rate of 1 / s where s is the number of parameters in the true model , whereas for MLE the rate is 1 / t , where t is the size of the domain , or in our context , the total number of elements of  . )
` Consistency ' is another desirable property of MDL , which is not shared by MLE .
That is , the number of parameters in the models selected by MDL converge to that of the true model Rissanen 1984 .
Both of these properties of MDL are empirically verified in our present context , as will be shown in the next section .
In particular , we have compared the performance of employing an MDL-based simulated annealing against that of one based on MLE in word clustering .
We describe our experimental results in this section .
We compared the performance of employing MDL as a criterion in our simulated annealing algorithm , against that of employing MLE by simulation experiments .
We artificially constructed a true model of word co-occurrence , and then generated data according to its distribution .
We then used the data to estimate a model ( clustering words ) , and measured the KL distance between the true model and the estimated model .
( The algorithm used for MLE was the same as that shown in Figure  , except the ` data description length ' replaces the ( total ) description length ' in Step 2 . )
Figure  plots the relation between the number of obtained noun clusters ( leaf nodes in the obtained thesaurus tree ) versus the input data size , averaged over 10 trials .
( The number of noun clusters in the true model is 4 . )
Figure   plots the KL distance versus the data size , also averaged over the same 10 trials .
The results indicate that MDL converges to the true model faster than MLE .
Also , MLE tends to select a model overfitting the data , while MDL tends to select a model which is simple and yet fits the data reasonably well .
We conducted the same simulation experiment for some other models and found the same tendencies .
( Figure   and Figure   show the analogous results when the number of noun clusters in the true model is 2 ) .
We conclude that it is better to employ MDL than MLE in word clustering .
We extracted roughly 180,000 case frames from the bracketed WSJ ( Wall Street Journal ) corpus of the Penn Tree Bank Marcus et al. 1993 as co-occurrence data .
We then constructed a number of thesauri based on these data , using our method .
Figure  shows an example thesaurus for the 20 most frequently occurred nouns in the data , constructed based on their appearances as subject and object of roughly 2000 verbs .
The obtained thesaurus seems to agree with human intuition to some degree .
For example , ` million ' and ` billion ' are classified in one noun cluster , and ` stock ' and ` share ' are classified together .
Not all of the noun clusters , however , seem to be meaningful in the useful sense .
This is probably because the data size we had was not large enough .
This general tendency is also observed in another example thesaurus obtained by our method , shown in Figure  .
Pragmatically speaking , however , whether the obtained thesaurus agrees with our intuition in itself is only of secondary concern , since the main purpose is to use the constructed thesaurus to help improve on a disambiguation task .
We also evaluated our method by using a constructed thesaurus in a pp-attachment disambiguation experiment .
We used as training data the same 180,000 case frames in Experiment 1 .
We also extracted as our test data 172  patterns from the data in the same corpus , which is not used in the training data .
For the 150 words that appear in the position of  , we constructed a thesaurus based on the co-occurrences between heads and slot values of the frames in the training data .
This is because in our disambiguation test we only need a thesaurus consisting of these 150 words .
We then applied the learning method proposed in Li and Abe 1995 to learn case frame patterns with the constructed thesaurus as input using the same training data .
That is , we used it to learn the conditional distributions  ,  , where  and  vary over the internal nodes in a certain ` cut ' in the thesaurus tree .
Table  shows some example case frame patterns obtained by this method , and Figure  shows the leaf nodes dominated by the internal nodes appearing in the case frame patterns of Table  .
We then compare  and  , which are estimated based on the case frame patterns , to determine the attachment site of  .
More specifically , if the former is larger than the latter , we attach it to verb , and if the latter is larger than the former , we attach it to  , and otherwise ( including when both are 0 ) , we conclude that we cannot make a decision .
Table  shows the results of our pp-attachment disambiguation experiment in terms of ` coverage ' and ` accuracy ' .
Here ` coverage ' refers to the proportion ( in percentage ) of the test patterns on which the disambiguation method could make a decision .
` Base Line ' refers to the method of always attaching  to  .
` Word-Based , ' ` MLE-Thesaurus , ' and ` MDL-Thesaurus ' respectively stand for using word-based estimates , using a thesaurus constructed by employing MLE , and using a thesaurus constructed by our method .
Note that the coverage of ` MDL-Thesaurus ' significantly outperformed that of ` Word-Based , ' while basically maintaining high accuracy ( though it drops somewhat ) , indicating that using an automatically constructed thesaurus can improve disambiguation results in terms of coverage .
We also tested the method proposed in Li and Abe 1995 of learning case frames patterns using an existing thesaurus .
In particular , we used this method with WordNet Miller et al. 1993 and using the same training data , and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns .
We show the result of this experiment as ` WordNet ' in Table  .
We can see that in terms of ` coverage , ' ` WordNet ' outperforms ` MDL-Thesaurus , ' but in terms of ` accuracy , ' ` MDL-Thesaurus ' outperforms ` WordNet ' .
These results can be interpreted as follows .
An automatically constructed thesaurus is more domain dependent and captures the domain dependent features better , and thus using it achieves high accuracy .
On the other hand , since training data we had available is insufficient , its coverage is smaller than that of a hand made thesaurus .
In practice , it makes sense to combine both types of thesauri .
More specifically , an automatically constructed thesaurus can be used within its coverage , and outside its coverage , a hand made thesaurus can be used .
Given the current state of the word clustering technique ( namely , it requires data size that is usually not available , and it tends to be computationally demanding ) , this strategy is practical .
We show the result of this combined method as ` MDL-Thesaurus + WordNet ' in Table  .
Our experimental result shows that employing the combined method does increase the coverage of disambiguation .
We also tested ` MDL-Thesaurus + WordNet + LA + Default , ' which stands for using the learned thesaurus and WordNet first , then the lexical association value proposed by Hindle and Rooth 1991 , and finally the default ( i.e. always attaching  to  ) .
Our best disambiguation result obtained using this last combined method somewhat improves the accuracy reported in Li and Abe 1995 (  ) .
We have proposed a method of clustering words based on large corpus data .
We conclude with the following remarks .
Our method of hierarchical clustering of words based on the MDL principle is theoretically sound .
Our experimental results show that it is better to employ MDL than MLE as estimation criterion in word clustering .
Using a thesaurus constructed by our method can improve pp-attachment disambiguation results .
At the current state of the art in statistical natural language processing , it is best to use a combination of an automatically constructed thesaurus and a hand made thesaurus for disambiguation purpose .
The disambiguation accuracy obtained this way was  .
In the future , hopefully with larger training data size , we plan to construct larger thesauri as well as to test other clustering algorithms .
We thank Mr. K. Nakamura , Mr. T. Fujita , and Dr. K. Kobayashi of NEC C &amp; C Res. Labs.  for their constant encouragement .
We thank Dr. K. Yamanishi of C &amp; C Res. Labs. for his comments .
We thank Ms. Y. Yamaguchi of NIS for her programming effort .
