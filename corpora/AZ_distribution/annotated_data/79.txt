In this paper I report on an investigation into the problem of assigning tones to pitch contours .
The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages .
Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang ( Cameroon ) .
Following recent work by Liberman and others , I provide a parametrised F  prediction function  which generates F  values from a tone sequence , and I explore the asymptotic behaviour of downstep .
Next , I observe that transcribing a sequence X of pitch ( i.e. F  ) values amounts to finding a tone sequence T such that  .
This is a combinatorial optimisation problem , for which two non-deterministic search techniques are provided : a genetic algorithm and a simulated annealing algorithm .
Finally , two implementations -- one for each technique -- are described and then compared using both artificial and real data for sequences of up to 20 tones .
These programs can be adapted to other tone languages by adjusting the F  prediction function .
The wealth of literature on tone and intonation has amply demonstrated that voice pitch ( F  ) in speech is under independent linguistic control .
In English , voice pitch alone can signal the distinction between a statement and a question .
Similarly , in many tone languages , voice pitch alone signals the tense of a verb .
Phonologists usually describe a pitch contour much as they describe speech more generally , namely as a sequence of discrete units ( i.e. a transcription ) .
This is illustrated in Figure  , where L indicates a low tone and  H indicates a downstepped high tone .
The question addressed in this paper concerns how we should relate pitch contours to tone sequences .
This paper is divided into four main sections , summarised in turn below .
Tone Transcription
In this section I present the problem of relating sequences of F  values to tone transcriptions .
I argue that Hidden Markov Models are unsuited to the task and I demonstrate the importance of having a computational tool which allows phonologists to experiment with F  scaling parameters .
F  Scaling
This section gives a mathematical basis for a general approach to F  scaling which , it is hoped , will be applicable to any tone language .
I derive an F  prediction function from first principles and show how the model of Liberman et al. 1993  for the Nigerian language Igbo is a special case .
Tone and F  in Bamileke Dschang
Here I present some data from my own fieldwork and give a statistical analysis , using the same technique used by Liberman et al. .
I then show how the general model of the previous section is instantiated for this language .
This demonstrates the versatility of the general model , since it can be applied to two very different tone languages .
Implementations
This section provides two non-deterministic techniques for transcribing an F  string .
The first method uses a genetic algorithm while the second method uses simulated annealing .
The performance of both implementations is evaluated and compared on a range of artificial and real data .
Finally , I give some examples of multiple , automatically-generated transcriptions of the same F  data .
A promising way of generating contours from tone sequences is to specify one or more pitch targets per tone and then to interpolate between the targets ; the task then becomes one of providing a suitable sequence of targets Pierrehumbert and Beckman 1988 .
It is perhaps less clear how we should go about recognising tone sequences from pitch contours .
Hidden Markov Models ( HMMs ) Huang et al. 1990 offer a powerful statistical approach to this problem , though it is unclear how they could be used to recognise the units of interest to phonologists .
HMMs do not encode timing information in a way that would allow them to output , say , one tone per syllable ( or vowel ) .
Moreover , the same section of a pitch contour may correspond to either H or L tones .
For example , a H between two Hs looks just like an L between two Ls .
There is no principled upper bound on the amount of context that needs to be inspected in order to resolve the ambiguity , leading to a multiplication of state information required by the HMM and problems for training it .
In the present context , the emphasis is not on automatic speech recognition but on a tool to support phonologists working with tone .
As we shall see in the next section , once the phonologist has identified the salient location to measure the ` F  value ' of a syllable ( or some other phonological unit ) , the task will be to automatically map a string of these values to a string of tones .
Connell and Ladd have devised a set of heuristics for identifying key points in an F  contour to record F  values Connell and Ladd 1990 , 21 ff .
In the absence of a program which enshrines these heuristics , it was decided to develop a system for producing a tone transcription from a sequence of F  values .
Apart from the obvious benefits of automating the process , such as speed and accuracy , it could show up cases where there is more than one possible tone transcription , possibly with different parameter settings for the F  scaling function .
Having the set of tone transcriptions that are compatible with an utterance has considerable value to an analyst searching for invariances in the tonal assignments to individual morphemes .
To exemplify this point , it is worth considering a recent example where an alternative transcription of some data proved valuable in providing a fresh analysis of the data .
In their analyses of tone in Bamileke Dschang , Hyman gives the transcription in  while Stewart gives the one in  , for the phrase meaning machete of dogs .
These two possibilities exist because of different F  scaling parameters .
These parameters determine the way in which the different tones are scaled relative to each other and to the speaker 's pitch range .
This is illustrated in  , adapting Hyman 's earlier notation Hyman 1979 .
Example  displays a kind of phonetic interpretation function .
Immediately below the two rows of tones we see a row of numbers corresponding to the tones .
For Hyman , L = 3 and H = 1 , while for Stewart , L = 2 and H = 1 .
Observe in Hyman 's example that a rising tone -- symbolised by a wedge above the i -- is modelled as an LH sequence in keeping with standard practice in African tone analysis .
The second row of numbers corresponds to downstep (  ) and upstep (  ) .
For Hyman 's model , this row begins at 0 and is increased by 1 for each downstep encountered .
For Stewart 's model , this row begins at 1 and is increased by 1 for each downstep encountered and decreased by 1 for each upstep encountered .
The two rows are summed vertically to give the last row of numbers .
Observe that the last rows of Stewart 's and Hyman 's models are identical .
The parameter which distinguishes the two approaches is partial vs. total downstep .
Hyman treats Dschang as a partial downstep language , i.e. where  H appears as a mid tone ( with respect to the material to its left ) .
Stewart treats it as a total downstep language , i.e. where  H appears as an L tone ( with respect to the material to its left ) .
While Hyman and Stewart present rather different analyses of rather different looking transcriptions , we can see that they are really analyzing the same data , given the above interpretation function .
Therefore , phonologists who do not wish to limit themselves to the transcriptions which result from certain parameter settings in the phonetic interpretation function would be better off working directly with number sequences like the last row in  .
This paper describes a tool which lets them do just that .
Consider again the F  contour in Figure  .
In particular , note that the F  decay seems to be to a non-zero asymptote , and that H and L appear to have different asymptotes which we symbolise as h and l respectively .
These observations are clearer in Figure  , which ( roughly speaking ) displays the peaks and valleys from Figure  .
Although this is admittedly a rather artificial example , it remains true that there is no principled upper limit on the number of downsteps that can occur in an utterance Clements 1979 , 540 , and so the asymptotic behaviour of F  scaling still needs to be addressed .
Now suppose that we have a sequence T of tones where  is the ith tone ( H or L ) and a sequence X of F  values where  is the F  value corresponding to  .
Then we would like a formula which predicts  given  ,  and  ( i > 1 ) .
We express this as follows :
The question , now , is what should this function look like ?
Suppose for sake of argument that the ratio of L to the immediately preceding H in Figure  is constant , with respect to the baselines for H and L , namely h and l .
Then we have :
More generally , suppose that we have a sequence of two arbitrary tones .
Ignoring the possibility of downstep for the present , we have a static two-tone system where HH and LL sequences are level and sequences like HLHLHL are realised as simple oscillation between two pitches .
We can write the following formula , where  if  and  if  .
The situation becomes more interesting when we allow for downdrift and downstep .
Downdrift is the automatic lowering of the second of two H tones when an L intervenes , so HLH is realised as  rather than as  , while downstep is the lowering of the second of two tones when an intervening L is lost , so H  H is realised as  Hyman and Schuh 1974 .
Bamileke Dschang has downstep but not downdrift while Igbo has downdrift but only very limited downstep .
Now we define  if  H ,  H and  if  L ,  L .
Generalising our equation once more , we have the following , where R is a factor called the transition ratio .
Now I shall show how this general equation relates to the equations for Igbo Liberman et al. 1993 , reproduced below :
 can be instantiated to the set of equations in  by setting R as follows :
It will be helpful to introduce one more level of generality .
 relates adjacent F  values , but we would also like to relate non-adjacent values , given the sequence of intervening tones .
Suppose that  is a tone sequence where the F  value of  is x .
Then we shall write the F  value of  as  .
By repeated applications of  we can write down the following expression for  :
where  , n  >  2 .
Now , suppose that  and  are tone sequences and that  ,  and  .
Then it is straightforward to show that  .
Notice also that if  for all x and if  then  .
These results will be useful in the next section .
Finally , it is worth comparing  with Hyman 's and Stewart 's interpretation functions which were illustrated in  .
As pointed out already , Hyman 's is a partial downstep model while Stewart 's is a total downstep model .
Partial and total downstep can be visualised as follows , where the dotted lines indicate the abstract register inside which tones are scaled , and where downstep corresponds to lowering of the register .
Observe that for partial downstep , it is necessary to have two downsteps before a high tone is at the level of a preceding low , while for total downstep , it is only necessary to have a single downstep for a high tone to be at the same level as the preceding low .
We can express these observations about partial and total downstep in the model as follows .
For partial downstep , we have  while for total downstep we have  .
For both of these equations we are forced to have h = l which does not seem to be empirically justifiable in view of the data in Figure  .
It might be argued that this indicates a flaw in the model being presented here , since partial and total downstep are widely attested in the literature on tone languages .
Unfortunately , it is not possible in general to provide a model for partial or total downstep which permits distinct asymptotes for H and L .
Therefore , to the extent that Figure  is typical of tone languages in having different H and L asymptotes , one must conclude that total and partial downstep are qualitative terms only .
However , they may yet re-emerge in the model under a different guise , as we shall see later .
The effect of the distinction between partial and total downstep is to allow different transcriptions of the same string , as we saw in  .
In general , we have the following mapping between transcriptions under the two views of downstep :
It is clear that changing from one view of downstep to the other amounts to adding and deleting  and  while leaving the tones themselves unchanged .
Thus , the model admits both transcription schemes that result from the two views of downstep , and another besides , as shown later in  .
This concludes the discussion of the F  prediction function .
In the next section I shall investigate the phonetic interpretation of tone in Bamileke Dschang , and determine the values of R for this language .
In a recent field trip to Western Cameroon to study the Bamileke Dschang noun associative construction , I was able to collect a small amount of data relating to F  scaling throughout a particular informant 's pitch range .
Following Liberman et al. , voice pitch was varied by getting the informant to speak at different volumes and by adjusting the recording level appropriately .
However , rather than asking the informant to imagine speaking to a subject at different distances , I controlled the volume by having the informant wear headphones and played white noise from a detuned radio .
Thus , I could set the informant 's voice pitch by using the volume control on my radio .
My hypothesis is that this technique produces more consistent volume ( and hence , pitch scaling ) over long utterances and may make informants less self-conscious about speaking loudly than simply asking them to imagine speaking to subjects at various distances away .
Measurements were taken from the following data .
Regrettably , the LL data was only available from isolated disyllables , and other sequences such as LH and H  H were not available at all .
However , from the F  data for the above utterances we can hypothesise the behaviour of these unseen sequences , and this can be tested in subsequent empirical work .
The results for utterances involving HH and LL sequences are displayed in Figure  , while results for L  H and HL are displayed in Figure  .
The regression equations obtained from these data are displayed in  , where the number of occurrences of each tone sequence is given in parentheses after the sequence .
The third column gives the standard error for the gradient and intercept .
From this , we conclude that HL is the only sequence with an intercept significantly different from zero , and that  for HH and LL sequences .
We also conclude that  , ( l / h = 1.1 ) and  .
This last value will be referred to as the quantity d .
We also see that l = 88 Hz and h = 96 Hz .
Fortunately , these figures are sufficient to determine the R values for all other pairs of tones in Bamileke Dschang .
A further observation is that Bamileke Dschang does not have downdrift , and so there is no F  difference across HLH and LHL sequences .
This is evident in Figure  .
Therefore , we can write  , and by a result we showed above ,  .
Given that  it follows that  .
Concerning downstep , I shall assume that the magnitude of downstep is independent of the tones on either side , and so  .
A separate instrumental study supports this hypothesis Bird and Stegen 1993 .
Therefore , we have  , where s is any tone and t is H or L .
Finally , it is important to briefly consider upstep , since it has been used in some analyses of Bamileke Dschang ( e.g. Stewart 's ) .
Given that upstep and downstep are intended as inverses of each other , we have the identities  , with s , t as before .
We now have a complete table for R :
Observe the symmetries in this table .
The configuration of four R values that we find when  is not downstepped or upstepped ( the first two columns ) is reproduced in the columns for downstep ( multiplied by d ) and in the columns for upstep ( divided by d ) .
Note also that the above table is dependent upon how the data in  was transcribed .
Suppose that we had not used repetitions of HL  H ( a transcription scheme based on partial downstep ) but H  LH ( a scheme based on total downstep ) .
Then we would have had  and  .
Accordingly , the table for R would be as follows :
The fact that we have two possible tables for R is no cause for alarm .
Recall that the transition between two tones  and  also involves the factor  .
This factor is manifested in tone transitions according to the following pattern :
I therefore conclude that the presence of more than one table for R indicates an interplay between R values and the ratio h / l .
This raises an interesting question .
Suppose we have two tone sequences  and  , and two interpretation functions  and  based on R and R ' respectively .
Then under what circumstances is the phonetic interpretation of both sequences the same under their respective interpretation functions ?
A sufficient condition for them to be the same is that  and that  .
The reader can check that these conditions are met by the mapping in  and the two tables for R given above .
Note that this observation holds for the model in general , not just for the specialised version of the model as applied to Bamileke Dschang .
It can also be shown that R is completely determined once  is specified .
A possible characterisation of total vs. partial downstep now arises : if  then we have total downstep , but if  then we have partial downstep .
However , the interpretation of these terms must necessarily be different from the standard interpretation , since I have shown that the standard interpretation is not compatible with the present model .
This concludes the discussion of F  scaling in Bamileke Dschang .
I shall now present the implementations .
In this section , I show how it is possible to get two programs to produce a sequence of tones T ( i.e. a tone transcription ) given a sequence of n F  values X .
The programs make crucial use of the prediction function  in evaluating candidate tone transcriptions .
Both programs involve search , and in general , the aim in searching is to discover the values for  so as to optimise the value of a specified evaluation function  .
When f has many local optima , deterministic methods such as hill-climbing perform poorly .
This is because they terminate in a local optimum and the particular one found depends heavily on the starting point in the search , and there is usually no way of choosing a good starting point .
Exhaustive search for the global optimum is not an option when the search space is prohibitively large .
In the present context , say for a sequence of 20 tones , the search space contains  possible tone transcriptions , and for each of these there are thousands of possible parameter settings , too large a search space for exhaustive search in a reasonable amount of computation time .
Non-deterministic search methods have been devised as a way of tackling large-scale combinatorial optimisation problems , problems that involve finding optima of functions of discrete variables .
These methods are only designed to yield an approximate solution , but they do so in a reasonable amount of computation time .
The best known such methods are genetic search Goldberg 1989 and annealing search van Laarhoven and Aarts 1987 .
Recently , annealing search has been successfully applied to the learning of phonological constraints expressed as finite-state automata Ellison 1992 .
In the following sections I describe a genetic algorithm and an annealing algorithm for the tone transcription problem .
For a cogent introduction to genetic search and an explanation of why it works , the reader is referred to South et al. 1993 .
Before presenting the version of the algorithm used in the implementation , I shall informally define the key data types it uses along with the standard operations on those types .
gene
A linear encoding of a solution .
In the present setting , it is an array of n tones , where each tone is one of H ,  H ,  H , L ,  L or  L .
A gene also contains 16 bit encodings of the parameters h , l and d .
These encodings were scaled to be floating point numbers in the range [ 90,110 ] for h , [ 70,100 ] for l and [ 0.6,0.9 ] for d .
gene pool
An array of genes , P .
One of the search parameters is the size of P , known as the population .
The gene pool is renewed each generation , and the number of generations is another search parameter .
evaluation
A measure of the fitness of a gene as a solution to the problem .
Suppose that X is the sequence of F  values we wish to transcribe .
Suppose also that T is a particular gene .
The the evaluation function is as follows :
crossover
This is an operation which takes two genes and produces a single gene as the result .
Suppose that  and  .
Then the crossover function  is defined as follows , where r is the ( randomly selected ) crossover point (  ) .
In other words , the genes A and B are cut at a position determined by r and the first part of A is spliced with the second part of B to create a new gene .
Crossover builds in the idea that good genes tend to produce good offspring .
To see why this is so , suppose that the transcription contained in the first part of A is relatively good while the rest is poor , while the transcription contained in the first part of B is poor and the rest is relatively good .
Then the offspring containing the first part of A and the second part of B will be an improvement on both A and B ; other possible offspring from A and B will be significantly worse and may not survive to the next generation .
The program performs this kind of crossover for the parameters h , l and d , employing independent crossover points for each , and randomising the argument order in  so that the high order bits in the offspring are equally likely to come from either parent .
An extension to crossover allows more than one crossing point .
The current model permits an arbitrary number of crossing points for crossover on the transcription string .
The resulting gene is optimal since we choose the crossing points in such a way as to minimise  at each position .
In developing the system , exploiting the decomposability of the evaluation function in this way caused a significant improvement in system performance over the version which used simple crossover .
breeding
For each generation , we create a new gene pool from the previous one .
Each new gene is created by mating the best of three randomly chosen genes with the best of three other randomly chosen genes .
mutation
In order to maintain some genetic diversity and an element of randomness throughout the search ( rather than just in the initial configuration ) , a further operation is applied to each gene in every generation .
With a certain probability ( known as the mutation probability ) , for each gene T and each tone in T , the tone is randomly set to any of the six possible tones .
Likewise , the parameter encodings are mutated .
The mutation rate is set to 0.005 but raised to 0.5 for a single generation if the evaluation of the best gene is no improvement on the evaluation of the best gene ten generations earlier .
The best gene is never mutated .
The building blocks of genetic search discussed above are structured into the following algorithm , expressed in pseudo-Pascal :
The main loop is executed for each generation .
Each time through this loop , the program checks performance over the last ten generations and if performance has been good , the mutation rate stays low , otherwise it is changed to high .
Then it copies the best gene to the new pool .
Now we reach the inner loop , which selects two genes , performs crossover , and mutates the result .
Next , the current pool is updated , an evaluation is performed , and the program continues with the next generation .
Once all the generations have been completed , the program displays the best gene from the final population and terminates .
As with genetic algorithms , simulated annealing van Laarhoven and Aarts 1987 is a combinatorial optimisation technique based on an analogy with a natural process .
Annealing is the heating and slow cooling of a solid which allows the formation of regular crystalline structure having a minimum of excess energy .
In its early stages when the temperature is high , annealing search resembles random search .
There is so much free energy in the system that a transition to a higher energy state is highly probable .
As the temperature decreases the search begins to resemble hill-climbing .
Now there is much less free energy and so transitions to higher energy states are less and less likely .
In what follows , I explain some of the parameters of annealing search as used in the current implementation .
temperature
At the start of the search the temperature , t is set to 1 .
During the search , the temperature is reduced at a rate set by the ` cooling rate ' parameter , until it reaches a value less than  .
perturbation
At each step of the search , the current state is perturbed by an amount which depends on the temperature .
The temperature determines the fraction of the search space that is covered by a single perturbation step .
For a tone sequence of length n , we randomly reset the worst n .
t tones according to  .
For the parameters we proceed as follows , here exemplified for h .
First , set  .
Now , add to h a random number in the range  and check that the result is still in the range  .
equilibrium
At each temperature , the system is required to reach ` thermal equilibrium ' before the temperature is lowered .
In the present context , equilibrium is reached if no more than one of the last eight perturbations yielded a new state that was accepted .
free energy function
This is the amount of available energy for transitions to higher energy states .
In the current system , it is the distribution - 1000.t.log ( p ) , where p is a uniform random variable in the range ( 0,1 ] .
If the energy difference  between an old and a new state is less than the available energy , then the transition is accepted .
The factor of 1000 is intended to scale the energy distribution to typical values of the evaluation function .
Now the algorithm itself is presented :
The program is made up of two loops .
The outer loop simply iterates through the temperature range , beginning with a temperature of 1 and steadily decreasing it until it gets very close to zero .
The nested loop performs the task of reaching thermal equilibrium at each temperature .
The first step is to perturb the previous transcription to make a new one .
Notice that the temperature t is a parameter of the perturb function .
Next , the difference  between the old and new evaluations is calculated .
If the new transcription has a better evaluation than the old one , then  is negative .
 is negative or
 is positive and there is sufficient free energy in the system to allow the worse transcription to be accepted .
Finally , we check if the new transcription is better than the best transcription found so far ( BestTrans ) and if so , we set BestTrans to be the new transcription .
Once equilibrium is reached , the current transcription is set to be the best transcription found so far , and the search continues .
Both the genetic and annealing search algorithms have been implemented in C++ .
In this section , the performance of the two implementations is compared .
Performance statistics are based on 1,200 executions of each program .
Search parameters were set so that each execution took around 5 seconds on a Sun Sparc 10 .
Three performance trials were undertaken .
In the first trial , both programs generated random sequences of tones , then computed the corresponding F  sequence using  , then set about transcribing the F  sequence .
Since these sequences were ideal , the best possible evaluation for a transcription was zero .
The performance of the programs could then be measured to see how close they came to finding the optimal solution .
Each program was tested on F  sequences of length 5 , 10 , 15 and 20 .
For each length , each program transcribed 100 randomly-generated sequences .
The results are displayed in Figure  .
Each pair of bars corresponds to a given transcription length .
The left member of each pair is for the genetic search program , while the right member is for the annealing search program .
The heavily shaded bars corresponding to evaluations less than 1 are the most important .
These indicate the number of times out of 100 that the programs found a transcription with an evaluation less than 1 .
This evaluation means that the average of the squared difference between the predicted F  values and the actual F  values was less than 1 Hz. Observe that the annealing search program performs significantly better in all cases .
Note that the mutation operation in the genetic search program treats each bit in the parameter encodings equally , while the perturbation operation in the annealing search program is sensitive to the distinction between more significant vs. less significant bits .
This may explain the better convergence behaviour of the annealing search .
Notice also in Figure  that performance does not degrade with transcription length as the length doubles from 10 to 20 .
This is probably because a randomly generated sequence will contain downsteps on every second tone ( on average ) causing a general downtrend in the F  values and severely limiting the combinatorial explosion of possible transcriptions .
Trial 2 was the same as trial 1 except that this time upstep was permitted as well .
The results are displayed in Figure  .
Again the annealing program fares better than the genetic program .
Consider again the bars corresponding to evaluations less than 1 .
For both programs , however , observe that the performance degrades more uniformly than in trial 1 , probably because the inclusion of upstep greatly increases the number of possible transcriptions ( and hence , the number of local optima ) .
The final trial involved real data , including data from the utterance given in Figure  .
This trial involved four subtrials .
The first and second had F  sequences of length 10 , while the third and fourth had length 18 and 19 .
The first and second sequences were taken by extracting the initial 10 F  values from the third and fourth sequences , thereby avoiding the asymptotic behaviour of the longer sequences .
The data is tabulated below , and it comes from the sentences in  .
Performance results are given in Figure  .
Notice that the interpretation of the shading in this figure is different from that in previous figures .
This is because evaluations near zero were less likely with real data .
In fact , the annealing program never found an evaluation less than 3 while the genetic program never found an evaluation less than 4 .
Since the programs performed about equally on finding transcriptions with an evaluation less than 7 , I shall display these transcriptions along with an indication of how many times each program found the transcription ( G = genetic , A = annealing ) .
I give transcriptions which occurred at least twice in one of the programs , during 100 executions of each .
The results from trial 1 deserve special attention .
In trial 1 , three transcriptions were found by both programs .
The best evaluations found are given below :
It is striking to note that the first two transcriptions above are what Hyman and Stewart ( respectively ) would have given as transcriptions for the abstract F  sequence 1 3 2 4 3 5 4 6 5 7 .
This is demonstrated in  .
The third transcription points to another possibility , given in  .
Therefore , there are encouraging signs that the program is living up to its promise of producing alternative , equally acceptable transcriptions , as desired from an analytical standpoint .
Although we have seen more than one transcription for a given F  sequence , it is inconvenient to be required to run the programs several times in order to see if more than one solution can be found .
Furthermore , the programs are designed not to get caught in local optima , which is a problem since interesting alternative transcriptions may actually be local optima .
Therefore , both programs are set up to report the k best solutions , where the user specifies the number of solutions desired .
The program ensures that the same area of the search space is not re-explored by subsequent searches .
This is done by defining a distance metric on transcriptions which counts the number of tones in one transcription that have to be changed in order to make it identical to the other transcription .
That part of the search space within a distance of n / 3 from any previously found solution is not explored again .
The programs give up before finding k solutions if 5 randomly generated transcriptions all fall within distance n / 3 of previous solutions .
Now , consider the following randomly generated sequence of tones :
The annealing program was set the task of finding ten transcriptions of this tone sequence .
The program was run only twice , and it reported the following solutions with evaluations less than or equal to 1 .
Both runnings of the program found the same solutions , and in the same order .
( Note that two transcriptions are taken to be the same if one or both begin with an initial upstep or downstep ; this has no effect on the phonetic interpretation ) .
In the following displays , the predicted F  values are given below each solution to facilitate comparison with the input sequence .
Since all executions to this point have been based on the first table of R values , it was decided to try a test with the second table of R values to see if the performance was different .
Interestingly , the third solution in both of the above executions was not found , though two new solutions were found .
Observe that the value of d in the above solutions clusters around 0.66 and 0.87 .
Similar clustering may be occurring with the ratio h/l .
However , an analysis of the relationship between the kinds of solutions found , the two R tables and the parameter values h , l and d has not been attempted .
It is rather unsatisfying that the performance of the two programs is heavily dependent on the setting of several search parameters , and it seems to be a combinatorial optimisation problem in itself to find good parameter settings .
My trial-and-error approach will not necessarily have found optimal parameter values , and so it would be premature to conclude from the performance comparison that annealing search is better than genetic search for the problem of tone transcription .
A more thoroughgoing comparison of these two approaches to the problem needs to be undertaken .
Since the parameters are continuous variables , and since the evaluation function -- which we could write as  -- is a smoothly continuous function in h , l , d , it would be worthwhile to try other ( deterministic ) search methods for optimising h , l and d , once a candidate tone transcription T has been found .
Finally , it would be interesting to integrate a system like either of the ones presented here into a speech workstation .
As the phonologist identifies salient points with a cursor the system would do the transcription , incrementally and interactively .
This paper began with a discussion of the problem of relating tone transcriptions to their physical counterparts , namely F  traces .
I showed that it is desirable for phonologists working on tone to use sequences of F  values as their primary data , rather than impressionistic transcriptions which make ( usually implicit ) assumptions about F  scaling .
I provided an F  prediction function  which estimated the F  value of a tone , given the F  value of the previous tone and the identities of the two tones .
I presented instrumental data from Bamileke Dschang and showed how the function could be specialised for this language .
The function was then incorporated into the evaluation functions of two implemented non-deterministic search algorithms .
The performance results were encouraging and demonstrate the promise of automated tone transcription .
This research is funded by the UK Economic and Social Research Council , under grant R00023 4439 A Computational Model for the Phonology-Phonetics Interface in Tone Languages .
I am indebted to SIL Cameroon for their logistical support on my field trip in September and October of 1993 , during which the data presented in the paper ( and much other data besides ) was gathered , and especially to Nancy Haynes , Gretchen Harro for helping me collect the data and Jean-Claude Gnintedem who endured many recording sessions .
I am grateful to John Coleman , Michael Gasser and Marie South for helpful comments on an earlier version of this paper .
The F  data was extracted using the ESPS Waves + package in the Edinburgh University Phonetics Laboratory .
