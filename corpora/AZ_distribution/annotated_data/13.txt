Why should computers interpret language incrementally ?
In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word .
However , possible computational applications have received less attention .
In this paper we consider various potential applications , in particular graphical interaction and dialogue .
We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations .
Finally , we tease apart the relationship between dynamic semantics and incremental interpretation .
Following the work of , for example , Marslen-Wilson 1973 , Just and Carpenter 1980 and Altmann and Steedman 1988 , it has become widely accepted that semantic interpretation in human sentence processing can occur before sentence boundaries and even before clausal boundaries .
It is less widely accepted that there is a need for incremental interpretation in computational applications .
In the 1970 s and early 1980 s several computational implementations motivated the use of incremental interpretation as a way of dealing with structural and lexical ambiguity ( a survey is given in Haddock 1989 ) .
A sentence such as the following has 4862 different syntactic parses due solely to attachment ambiguity Stabler 1991 .
Although some of the parses can be ruled out using structural preferences during parsing ( such as Late Closure or Minimal Attachment Frazier 1979 ) , extraction of the correct set of plausible readings requires use of real world knowledge .
Incremental interpretation allows on-line semantic filtering , i.e. parses of initial fragments which have an implausible or anomalous interpretation are rejected , thereby preventing ambiguities from multiplying as the parse proceeds .
However , on-line semantic filtering for sentence processing does have drawbacks .
Firstly , for sentence processing using a serial architecture ( rather than one in which syntactic and semantic processing is performed in parallel ) , the savings in computation obtained from on-line filtering have to be balanced against the additional costs of performing semantic computations for parses of fragments which would eventually be ruled out anyway from purely syntactic considerations .
Moreover , there are now relatively sophisticated ways of packing ambiguities during parsing ( e.g. by the use of graph-structured stacks and packed parse forests Tomita 1985 ) .
Secondly , the task of judging plausibility or anomaly according to context and real world knowledge is a difficult problem , except in some very limited domains .
In contrast , statistical techniques using lexeme co-occurrence provide a relatively simple mechanism which can imitate semantic filtering in many cases .
For example , instead of judging bank as a financial institution as more plausible than bank as a riverbank in the noun phrase the rich bank , we can compare the number of co-occurrences of the lexemes rich and bank  ( = riverbank ) versus rich and bank  ( = financial institution ) in a semantically analysed corpus .
Cases where statistical techniques seem less appropriate are where plausibility is affected by local context .
For example , consider the ambiguous sentence ,
in the two contexts
Such cases involve reasoning with an interpretation in its immediate context , as opposed to purely judging the likelihood of a particular linguistic expression in a given application domain ( see e.g. Cooper 1993 for discussion ) .
Although the usefulness of on-line semantic filtering during the processing of complete sentences is debatable , filtering has a more plausible role to play in interactive , real-time environments , such as interactive spell checkers ( see e.g. Wirn 1990 for arguments for incremental parsing in such environments ) .
Here the choice is between whether or not to have semantic filtering at all , rather than whether to do it on-line , or at the end of the sentence .
The concentration in early literature on using incremental interpretation for semantic filtering has perhaps distracted from some other applications which provide less controversial applications .
We will consider two in detail here : graphical interfaces , and dialogue .
The Foundations for Intelligent Graphics Project ( FIG ) considered various ways in which natural language input could be used within computer aided design systems ( the particular application studied was computer aided kitchen design , where users would not necessarily be professional designers ) .
Incremental interpretation was considered to be useful in enabling immediate visual feedback .
Visual feedback could be used to provide confirmation ( for example , by highlighting an object referred to by a successful definite description ) , or it could be used to give the user an improved chance of achieving successful reference .
For example , if sets of possible referents for a definite noun phrase are highlighted during word by word processing then the user knows how much or how little information is required for successful reference .
Human dialogue , in particular , task oriented dialogue is characterised by a large numbers of self-repairs  Levelt 1983 , Carletta et al. 1993  , such as hesitations , insertions , and replacements .
It is also common to find interruptions requesting extra clarification , or disagreements before the end of a sentence .
It is even possible for sentences started by one dialogue participant to be finished by another .
Applications involving the understanding of dialogues include information extraction from conversational databases , or computer monitoring of conversations .
It also may be useful to include some features of human dialogue in man-machine dialogue .
For example , interruptions can be used for early signalling of errors and ambiguities .
Let us first consider some examples of self-repair .
Insertions add extra information , usually modifiers e.g.
Replacements correct pieces of information e.g.
In some cases information from the corrected material is incorporated into the final message .
For example , consider :
In  , the corrected material the three main sources of data come provides the antecedent for the pronoun they .
In  the corrected material tells us that the man is both old and has a wife .
In  , the pronoun he is bound by the quantifier every boy .
For a system to understand dialogues involving self-repairs such as those in  would seem to require either an ability to interpret incrementally , or the use of a grammar which includes self repair as a syntactic construction akin to non-constituent coordination ( the relationship between coordination and self-correction is noted by Levelt 1983 ) .
For a system to generate self repairs might also require incremental interpretation , assuming a process where the system performs on-line monitoring of its output ( akin to Levelt 's model of the human self-repair mechanism ) .
It has been suggested that generation of self repairs is useful in cases where there are severe time constraints , or where there is rapidly changing background information Carletta p.c. .
A more compelling argument for incremental interpretation is provided by considering dialogues involving interruptions .
Consider the following dialogue from the TRAINS corpus Gross et al. 1993 .
This requires interpretation by speaker B before the end of A 's sentence to allow objection to the apposition , the engine at Avon , engine E .
An example of the potential use of interruptions in human computer interaction is the following :
In this example , interpretation must not only be before the end of the sentence , but before a constituent boundary ( the verb phrase in the user 's command has not yet been completed ) .
In this section we shall briefly review work on providing semantic representations ( e.g. lambda expressions ) word by word .
Traditional layered models of sentence processing first build a full syntax tree for a sentence , and then extract a semantic representation from this .
To adapt this to an incremental perspective , we need to be able to provide syntactic structures ( of some sort ) for fragments of sentences , and be able to extract semantic representations from these .
One possibility , which has been explored mainly within the Categorial Grammar tradition Steedman 1988 is to provide a grammar which can treat most if not all initial fragments as constituents .
They then have full syntax trees from which the semantics can be calculated .
However , an alternative possibility is to directly link the partial syntax trees which can be formed for non-constituents with functional semantic representations .
For example , a fragment missing a noun phrase such as John likes can be associated with a semantics which is a function from entities to truth values .
Hence , the partial syntax tree given in Fig.  ,
 can be associated with a semantic representation ,  x. likes ( john , x ) .
Both Categorial approaches to incremental interpretation and approaches which use partial syntax trees get into difficulty in cases of left recursion .
Consider the sentence fragment , Mary thinks John .
A possible partial syntax tree is provided by Fig.  .
However , this is not the only possible partial tree .
In fact there are infinitely many different trees possible .
The completed sentence may have an arbitrarily large number of intermediate nodes between the lower s node and the lower np .
For example , John could be embedded within a gerund e.g. Mary thinks John leaving here was a mistake , and this in turn could be embedded e.g. Mary thinks John leaving here being a mistake is surprising .
John could also be embedded within a sentence which has a sentence modifier requiring its own s node e.g. Mary thinks John will go home probably , and this can be further embedded e.g. Mary thinks John will go home probably because he is tired .
The problem of there being an arbitrary number of different partial trees for a particular fragment is reflected in most current approaches to incremental interpretation being either incomplete , or not fully word by word .
For example , incomplete parsers have been proposed by Stabler 1991 and Moortgat 1988 .
Stabler 's system is a simple top-down parser which does not deal with left recursive grammars .
Moortgat 's M-System is based on the Lambek Calculus : the problem of an infinite number of possible tree fragments is replaced by a corresponding problem of initial fragments having an infinite number of possible types .
A complete incremental parser , which is not fully word by word , was proposed by Pulman 1986 .
This is based on arc-eager left-corner parsing Resnik 1992 .
To enable complete , fully word by word parsing requires a way of encoding an infinite number of partial trees .
There are several possibilities .
The first is to use a language describing trees where we can express the fact that John is dominated by the s node , but do not have to specify what it is immediately dominated by ( e.g. D-Theory ) Marcus et al. 1983 .
Semantic representations could be formed word by word by extracting ` default ' syntax trees ( by strengthening dominance links into immediated dominance links wherever possible ) .
A second possibility is to factor out recursive structures from a grammar .
Thompson et al. 1991 show how this can be done for a phrase structure grammar ( creating an equivalent Tree Adjoining Grammar Joshi 1987 ) .
The parser for the resulting grammar allows linear parsing for an ( infinitely ) parallel system , with the absorption of each word performed in constant time .
At each choice point , there are only a finite number of possible new partial TAG trees ( the TAG trees represents the possibly infinite number of trees which can be formed using adjunction ) .
It should again be possible to extract ` default ' semantic values , by taking the semantics from the TAG tree ( i.e. by assuming that there are to be no adjunctions ) .
A somewhat similar system has recently been proposed by Shieber and Johnson 1993 .
The third possibility is suggested by considering the semantic representations which are appropriate during a word by word parse .
Although there are any number of different partial trees for the fragment Mary thinks John , the semantics of the fragment can be represented using just two lambda expressions :
Consider the first .
The lambda abstraction ( over a functional item of type e  t ) can be thought of as a way of encoding an infinite set of partial semantic ( tree ) structures .
For example , the eventual semantic structure may embed john at any depth e.g.
The second expression ( a functional item over type e  t and t  t ) , allows for eventual structures where the main sentence is embedded e.g.
This third possibility is therefore to provide a syntactic correlate of lambda expressions .
In practice , however , provided we are only interested in mapping from a string of words to a semantic representation , and don't need explicit syntax trees to be constructed , we can merely use the types of the ` syntactic lambda expressions ' , rather than the expressions themselves .
This is essentially the approach taken in Milward 1992 in order to provide complete , word by word , incremental interpretation using simple lexicalised grammars , such as a lexicalised version of formal dependency grammar and simple categorial grammar .
In processing the sentence Mary introduced John to Susan , a word-by-word approach such as Milward 1992 provides the following logical forms after the corresponding sentence fragments are absorbed :
Each input level representation is appropriate for the meaning of an incomplete sentence , being either a proposition or a function into a proposition .
In Chater et al. 1994 it is argued that the incrementally derived meanings are not judged for plausibility directly , but instead are first turned into existentially quantified propositions .
For example , instead of judging the plausibility of  , we judge the plausibility of  .
This is just the proposition Mary introduced something to something using a generalized quantifier notation of the form Quantifier ( Variable , Restrictor , Body ) .
Although the lambda expressions are built up monotonically , word by word , the propositions formed from them may need to be retracted , along with all the resulting inferences .
For example , Mary introduced something to something is inappropriate if the final sentence is Mary introduced noone to anybody .
A rough algorithm is as follows :
If P  does not entail P  retract P  and all conclusions made from it .
If implausible block this derivation .
It is worth noting that the need for retraction is not due to a failure to extract the correct ` least commitment ' proposition from the semantic content of the fragment Mary introduced .
This is due to the fact that it is possible to find pairs of possible continuations which are the negation of each other ( e.g. Mary introduced noone to anybody and Mary introduced someone to somebody ) .
The only proposition compatible with both a proposition , p , and its negation ,  p is the trivial proposition , T ( see Chater et al. for further discussion ) .
So far we have only considered semantic representations which do not involve quantifiers ( except for the existential quantifier introduced by the mechanism above ) .
In sentences with two or more quantifiers , there is generally an ambiguity concerning which quantifier has wider scope .
For example , in sentence  below the preferred reading is for the same kid to have climbed every tree ( i.e. the universal quantifier is within the scope of the existential ) whereas in sentence  the preferred reading is where the universal quantifier has scope over the existential .
Scope preferences sometimes seem to be established before the end of a sentence .
For example , in sentence  below , there seems a preference for an outer scope reading for the first quantifier as soon as we interpret child .
In  the preference , by the time we get to e.g. grammar , is for an inner scope reading for the first quantifier .
This intuitive evidence can be backed up by considering garden path effects with quantifier scope ambiguities ( called jungle paths by Barwise 1987 ) .
The original examples , such as the following ,
showed that preferences for a particular scope are established and are overturned .
To show that preferences are sometimes established before the end of a sentence , and before a potential sentence end , we need to show garden path effects in examples such as the following :
Most psycholinguistic experimentation has been concerned with which scope preferences are made , rather than the point at which the preferences are established Kurtzman and MacDonald 1993 .
Given the intuitive evidence , our hypothesis is that scope preferences can sometimes be established early , before the end of a sentence .
This leaves open the possibility that in other cases , where the scoping information is not particularly of interest to the hearer , preferences are determined late , if at all .
Dealing with quantifiers incrementally is a rather similar problem to dealing with fragments of trees incrementally .
Just as it is impossible to predict the level of embedding of a noun phrase such as John from the fragment Mary thinks John , it is also impossible to predict the scope of a quantifier in a fragment with respect to the arbitrarily large number of quantifiers which might appear later in the sentence .
Again the problem can be avoided by a form of packing .
A particularly simple way of doing this is to use unscoped logical forms where quantifiers are left in situ ( similar to the representations used by Hobbs and Shieber 1987 , or to Quasi Logical Form Alshawi 1990 ) .
For example , the fragment Every man gives a book can be given the following representation :
Each quantified term consists of a quantifier , a variable and a restrictor , but no body .
To convert lambda expressions to unscoped propositions , we replace an occurrence of each argument with an empty existential quantifier term .
In this case we obtain :
Scoped propositions can then be obtained by using an outside-in quantifier scoping algorithm Lewin 1990 , or an inside-out algorithm with a free variable constraint Hobbs and Shieber 1987 .
The propositions formed can then be judged for plausibility .
To imitate jungle path phenomena , these plausibility judgements need to feed back into the scoping procedure for the next fragment .
For example , if every man is taken to be scoped outside a book after processing the fragment Every man gave a book , then this preference should be preserved when determining the scope for the full sentence Every man gave a book to a child .
Thus instead of doing all quantifier scoping at the end of the sentence , each new quantifier is scoped relative to the existing quantifiers ( and operators such as negation , intensional verbs etc ) .
A preliminary implementation achieves this by annotating the semantic representations with node names , and recording which quantifiers are ` discharged ' at which nodes , and in which order .
Dynamic semantics adopts the view that `` the meaning of a sentence does not lie in its truth conditions , but rather in the way in which it changes ( the representation of ) the information of the interpreter '' Groenendijk and Stokhof 1991 .
At first glance such a view seems ideally suited to incremental interpretation .
Indeed , Groenendijk and Stokhof claim that the compositional nature of Dynamic Predicate Logic enables one to `` interpret a text in an on-line manner , i.e. , incrementally , processing and interpreting each basic unit as it comes along , in the context created by the interpretation of the text so far '' .
Putting these two quotes together is , however , misleading , since it suggests a more direct mapping between incremental semantics and dynamic semantics than is actually possible .
In an incremental semantics , we would expect the information state of an interpreter to be updated word by word .
In contrast , in dynamic semantics , the order in which states are updated is determined by semantic structure , not by left-to-right order ( see e.g. Lewin 1992 for discussion ) .
For example , in Dynamic Predicate Logic Groenendijk and Stokhof 1991 , states are threaded from the antecedent of a conditional into the consequent , and from a restrictor of a quantifier into the body .
Thus , in interpreting ,
the input state for evaluation of John will buy it right away is the output state from the antecedent a car impresses him .
In this case the threading through semantic structure is in the opposite order to the order in which the two clauses appear in the sentence .
Some intuitive justification for the direction of threading in dynamic semantics is provided by considering appropriate orders for evaluation of propositions against a database : the natural order in which to evaluate a conditional is first to add the antecedent , and then see if the consequent can be proven .
It is only at the sentence level in simple narrative texts that the presentation order and the natural order of evaluation necessarily coincide .
The ordering of anaphors and their antecedents is often used informally to justify left-to-right threading or threading through semantic structure .
However , threading from left-to-right disallows examples of optional cataphora , as in example  , and examples of compulsory cataphora as in :
Similarly , threading from the antecedents of conditionals into the consequent fails for examples such as :
It is also possible to get sentences with ` donkey ' readings , but where the indefinite is in the consequent :
This sentence seems to get a reading where we are not talking about a particular student ( an outer existential ) , or about a typical student ( a generic reading ) .
Moreover , as noted by Zeevat 1990 , the use of any kind of ordered threading will tend to fail for Bach-Peters sentences , such as :
For this kind of example , it is still possible to use a standard dynamic semantics , but only if there is some prior level of reference resolution which reorders the antecedents and anaphors appropriately .
For example , if  is converted into the ` donkey ' sentence :
When we consider threading of possible worlds , as in Update Semantics Veltman 1990 , the need to distinguish between the order of evaluation and the order of presentation becomes more clear cut .
Consider trying to perform threading in left-to-right order during interpretation of the sentence , John left if Mary left .
After processing the proposition John left the set of worlds is refined down to those worlds in which John left .
Now consider processing if Mary left .
Here we want to reintroduce some worlds , those in which neither Mary or John left .
However , this is not allowed by Update Semantics which is eliminative : each new piece of information can only further refine the set of worlds .
It is worth noting that the difficulties in trying to combine eliminative semantics with left-to-right threading apply to constraint-based semantics as well as to Update Semantics .
Haddock 1987 uses incremental refinement of sets of possible referents .
For example , the effect of processing the rabbit in the noun phrase the rabbit in the hat is to provide a set of all rabbits .
The processing of in refines this set to rabbits which are in something .
Finally , processing of the hat refines the set to rabbits which are in a hat .
However , now consider processing the rabbit in none of the boxes .
By the time the rabbit in has been processed , the only rabbits remaining in consideration are rabbits which are in something .
This incorrectly rules out the possibility of the noun phrase referring to a rabbit which is in nothing at all .
The case is actually a parallel to the earlier example of Mary introduced someone to something being inappropriate if the final sentence is Mary introduced noone to anybody .
Although this discussion has argued that it is not possible to thread the states which are used by a dynamic or eliminative semantics from left to right , word by word , this should not be taken as an argument against the use of such a semantics in incremental interpretation .
What is required is a slightly more indirect approach .
In the present implementation , semantic structures ( akin to logical forms ) are built word by word , and each structure is then evaluated independently using a dynamic semantics ( with threading performed according to the structure of the logical form ) .
At present there is a limited implementation , which performs a mapping from sentence fragments to fully scoped logical representations .
To illustrate its operation , consider the following discourse :
We assume that the first sentence has been processed , and concentrate on processing the fragment .
The implementation consists of five modules :
A word-by-word incremental parser for a lexicalised version of dependency grammar Milward 1992 .
This takes fragments of sentences and maps them to unscoped logical forms .
A module which replaces lambda abstracted variables with existential quantifiers in situ .
A pronoun coindexing procedure which replaces pronoun variables with a variable from the same sentence , or from the preceding context .
An outside-in quantifier scoping algorithm based on Lewin 1990 .
An ` evaluation ' procedure based on Lewin 1992 , which takes a logical form containing free variables ( such as the w in the LF above ) , and evaluates it using a dynamic semantics in the context given by the preceding sentences .
The output is a new logical form representing the context as a whole , with all variables correctly bound .
At present , the coverage of module  is limited , and module  is a naive coindexing procedure which allows a pronoun to be coindexed with any quantified variable or proper noun in the context or the current sentence .
The paper described some potential applications of incremental interpretation .
It then described the series of steps required in mapping from initial fragments of sentences to propositions which can be judged for plausibility .
Finally , it argued that the apparently close relationship between the states used in incremental semantics and dynamic semantics fails to hold below the sentence level , and briefly presented a more indirect way of using dynamic semantics in incremental interpretation .
