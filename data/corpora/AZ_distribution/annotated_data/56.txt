This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis .
The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) .
Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE .
The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts .
A text is not just a sequence of words , but it also has coherent structure .
The meaning of each word in a text depends on the structure of the text .
Recognizing the structure of text is an essential task in text understanding Grosz and Sidner 1986 .
One of the valuable indicators of the structure of text is lexical cohesion Halliday and Hasan 1976 .
Lexical cohesion is the relationship between words , classified as follows :
Reiteration :
Semantic relation :
Reiteration of words is easy to capture by morphological analysis .
Semantic relation between words , which is the focus of this paper , is hard to recognize by computers .
We consider lexical cohesion as semantic similarity between words .
Similarity is computed by spreading activation ( or association ) Waltz and Pollack 1985 on a semantic network constructed systematically from an English dictionary .
Whereas it is edited by some lexicographers , a dictionary is a set of associative relation shared by the people in a linguistic community .
The similarity between words is a mapping  :  , where L is a set of words ( or lexicon ) .
The following examples suggest the feature of the similarity :
The value of  increases with strength of semantic relation between w and w ' .
The following section examines related work in order to clarify the nature of the semantic similarity .
Section  describes how the semantic network is systematically constructed from the English dictionary .
Section  explains how to measure the similarity by spreading activation on the semantic network .
Section  shows applications of the similarity measure -- computing similarity between texts , and measuring coherence of a text .
Section  discusses the theoretical aspects of the similarity .
Words in a language are organized by two kinds of relationship .
One is a syntagmatic relation : how the words are arranged in sequential texts .
The other is a paradigmatic relation : how the words are associated with each other .
Similarity between words can be defined by either a syntagmatic or a paradigmatic relation .
Syntagmatic similarity is based on co-occurrence data extracted from corpora Church and Hanks 1990 , definitions in dictionaries Wilks et al. 1989 , and so on .
Paradigmatic similarity is based on association data extracted from thesauri Morris and Hirst 1991 , psychological experiments Osgood 1952 , and so on .
This paper concentrates on paradigmatic similarity , because a paradigmatic relation can be established both inside a sentence and across sentence boundaries , while syntagmatic relations can be seen mainly inside a sentence -- like syntax deals with sentence structure .
The rest of this section focuses on two related works on measuring paradigmatic similarity -- a psycholinguistic approach and a thesaurus-based approach .
Psycholinguists have been proposed methods for measuring similarity .
One of the pioneering works is ` semantic differential ' Osgood 1952 which analyses meaning of words into a range of different dimensions with the opposed adjectives at both ends ( see Figure  ) , and locates the words in the semantic space .
Recent works on knowledge representation are somewhat related to Osgood 's semantic differential .
Most of them describe meaning of words using special symbols like microfeatures Waltz and Pollack 1985 , Hendler 1989 that correspond to the semantic dimensions .
However , the following problems arise from the semantic differential procedure as measurement of meaning .
The procedure is not based on the denotative meaning of a word , but only on the connotative emotions attached to the word ; it is difficult to choose the relevant dimensions , i.e. the dimensions required for the sufficient semantic space .
Morris and Hirst 1991 used Roget 's thesaurus as knowledge base for determining whether or not two words are semantically related .
For example , the semantic relation of truck / car and drive / car are captured in the following way :
This method can capture almost all types of semantic relations ( except emotional and situational relation ) , such as paraphrasing by superordinate ( ex . cat / pet ) , systematic relation ( ex .  north / east ) , and non-systematic relation ( ex .  theatre / film ) .
However , thesauri provide neither information about semantic difference between words juxtaposed in a category , nor about strength of the semantic relation between words -- both are to be dealt in this paper .
The reason is that thesauri are designed to help writers find relevant words , not to provide the meaning of words .
We analyse word meaning in terms of the semantic space defined by a semantic network , called Paradigme .
Paradigme is systematically constructed from Glossme , a subset of an English dictionary .
A dictionary is a closed paraphrasing system of natural language .
Each of its headwords is defined by a phrase which is composed of the headwords and their derivations .
A dictionary , viewed as a whole , looks like a tangled network of words .
We adopted Longman Dictionary of Contemporary English  LDOCE 1987 as such a closed system of English .
LDOCE has a unique feature that each of its 56,000 headwords is defined by using the words in Longman Defining Vocabulary ( hereafter , LDV ) and their derivations .
LDV consists of 2,851 words ( as the headwords in LDOCE ) based on the survey of restricted vocabulary West 1953 .
We made a reduced version of LDOCE , called Glossme .
Glossme has every entry of LDOCE whose headword is included in LDV .
Thus , LDV is defined by Glossme , and Glossme is composed of LDV .
Glossme is a closed subsystem of English .
Glossme has 2,851 entries that consist of 101,861 words ( 35.73 words / entry on the average ) .
An item of Glossme has a headword , a word-class , and one or more units corresponding to numbered definitions in the entry of LDOCE .
Each unit has one head-part and several det-parts .
The head-part is the first phrase in the definition , which describes the broader meaning of the headword .
The det-parts restrict the meaning of the head-part .
( See Figure  . )
We then translated Glossme into a semantic network Paradigme .
Each entry in Glossme is mapped onto a node in Paradigme .
Paradigme has 2,851 nodes and 295,914 unnamed links between the nodes ( 103.79 links / node on the average ) .
Figure  shows a sample node red_1 .
Each node consists of a headword , a word-class , an activity-value , and two sets of links : a rfrant and a rfr .
A rfrant of a node consists of several subrfrants correspond to the units of Glossme .
As shown in Figure  and  , a morphological analysis maps the word brownish in the second unit onto a link to the node brown_1 , and the word colour onto two links to colour_1 ( adjective ) and colour_2 ( noun ) .
A rfr of a node p records the nodes referring to p .
For example , the rfr of red_1 is a set of links to nodes ( ex. apple_1 ) that have a link to red_1 in their rfrants .
The rfr provides information about the extension of red_1 , not the intension shown in the rfrant .
Each link has thickness  , which is computed from the frequency of the word  in Glossme and other information , and normalized as  in each subrfrant or rfr .
Each subrfrant also has thickness ( for example , 0.333333 in the first subrfrant of red_1 ) , which is computed by the order of the units which represents significance of the definitions .
Appendix A describes the structure of Paradigme in detail .
Similarity between words is computed by spreading activation on Paradigme .
Each of its nodes can hold activity , and it moves through the links .
Each node computes its activity value  at time  as follows :
where  and  are the sum of weighted activity ( at time T ) of the nodes referred in the rfrant and rfr respectively .
And ,  is activity given from outside ( at time T ) ; to ` activate a node ' is to let  .
The output function  sums up three activity values in appropriate proportion and limits the output value to [ 0,1 ] .
Appendix B gives the details of the spreading activation .
Activating a node for a certain period of time causes the activity to spread over Paradigme and produce an activated pattern on it .
The activated pattern approximately gets equilibrium after 10 steps , whereas it will never reach the actual equilibrium .
The pattern thus produced represents the meaning of the node or of the words related to the node by morphological analysis .
The activated pattern , produced from a word w , suggests similarity between w and any headword in LDV .
The similarity  is computed in the following way .
( See also Figure  )
Reset activity of all nodes in Paradigme .
Activate w with strength s(w) for 10 steps , where s(w) is significance of the word w.
Then , an activated pattern P(w) is produced on Paradigme .
Observe  -- an activity value of the node w ' in P(w) .
Then ,  is  .
The word significance  is defined as the normalized information of the word w in the corpus West 1953 .
For example , the word red appears 2,308 times in the 5,487,056 - word corpus , and the word and appears 106,064 times .
So ,  and  are computed as follows :
We estimated the significance of the words excluded from the word list West 1953 at the average significance of their word classes .
This interpolation virtually enlarged West 's 5,000,000 - word corpus .
For example , let us consider the similarity between red and orange .
First , we produce an activated pattern  on Paradigme .
( See Figure  . ) In this case , both of the nodes red_1 ( adjective ) and red_2 ( noun ) are activated with strength  .
Next , we compute  , and observe  .
Then , the similarity between red and orange is obtained as follows :
The procedure described above can compute the similarity  between any two words w , w ' in LDV and their derivations .
Computer programs of this procedure -- spreading activation ( in C ) , morphological analysis and others ( in Common Lisp ) -- can compute  within 2.5 seconds on a workstation ( SPARCstation 2 ) .
The similarity  between words works as an indicator of the lexical cohesion .
The following examples illustrate that  increases with the strength of semantic relation :
The similarity  also increases with the co-occurrence tendency of words , for example :
Note that  has direction ( from w to w ' ) , so that  may not be equal to  :
Meaningful words should have higher similarity ; meaningless words ( especially , function words ) should have lower similarity .
The similarity  increases with the significance s(w) and s ( w ' ) that represent meaningfulness of w and w ' :
Note that the reflective similarity  also depends on the significance s(w) , so that  :
The similarity of words in LDV and their derivations is measured directly on Paradigme ; the similarity of extra words is measured indirectly on Paradigme by treating an extra word as a word list  of its definition in LDOCE .
( Note that each  is included in LDV or their derivations . )
The similarity between the word lists W , W ' is defined as follows .
( See also Figure  . )
where P(w) is the activated pattern produced from W by activating each  with strength  for 10 steps .
And ,  is an output function which limits the value to [ 0,1 ] .
As shown in Figure  , bottle_1 and wine_1 have high activity in the pattern produced from the phrase `` red alcoholic drink '' .
So , we may say that the overlapped pattern implies `` a bottle of wine '' .
For example , the similarity between linguistics and stylistics , both are the extra words , is computed as follows :
Obviously , both  and  , where W is an extra word and w is not , are also computable .
Therefore , we can compute the similarity between any two headwords in LDOCE and their derivations .
This section shows the application of the similarity between words to text analysis -- measuring similarity between texts , and measuring text coherence .
Suppose a text is a word list without syntactic structure .
Then , the similarity  between two texts X , X ' can be computed as the similarity of extra words described above .
The following examples suggest that the similarity between texts indicates the strength of coherence relation between them :
It is worth noting that meaningless iteration of words ( especially , of function words ) has less influence on the text similarity :
The text similarity provides a semantic space for text retrieval -- to recall the most similar text in  to the given text X .
Once the activated pattern P ( X ) of the text X is produced on Paradigme , we can compute and compare the similarity  immediately .
( See Figure  . ) .
Let us consider the reflective similarity  of a text X , and use the notation c ( X ) for  .
Then , c ( X ) can be computed as follows :
The activated pattern P ( X ) , as shown in Figure  , represents the average meaning of  .
So , c ( X ) represents cohesiveness of X -- or semantic closeness of  , or semantic compactness of X .
( It is also closely related to distortion in clustering . )
The following examples suggest that c ( X ) indicates the strength of coherence of X :
= 0.502510 ( coherent ) ,
= 0.250840 ( incoherent ) .
However , a cohesive text can be incoherent ; the following example shows cohesiveness of the incoherent text -- three sentences randomly selected from LDOCE :
= 0.560172 ( incoherent , but cohesive ) .
Thus , c ( X ) can not capture all the aspects of text coherence .
This is because c ( X ) is based only on the lexical cohesion of the words in X .
The structure of Paradigme represents the knowledge system of English , and an activated state produced on it represents word meaning .
This section discusses the nature of the structure and states of Paradigme , and also the nature of the similarity computed on it .
The set of all the possible activated patterns produced on Paradigme can be considered as a semantic space where each state is represented as a point .
The semantic space is a 2,851 - dimensional hypercube ; each of its edges corresponds to a word in LDV .
LDV is selected according to the following information : the word frequency in written English , and the range of contexts in which each word appears .
So , LDV has a potential for covering all the concepts commonly found in the world .
This implies the completeness of LDV as dimensions of the semantic space .
Osgood 's semantic differential procedure used 50 adjective dimensions ; our semantic measurement uses 2,851 dimensions with completeness and objectivity .
Our method can be applied to construct a semantic network from an ordinary dictionary whose defining vocabulary is not restricted .
Such a network , however , is too large to spread activity over it .
Paradigme is the small and complete network for measuring the similarity .
The proposed similarity is based only on the denotational and intensional definitions in the dictionary LDOCE .
Lack of the connotational and extensional knowledge causes some unexpected results of measuring the similarity .
For example , consider the following similarity :
This is due to the nature of the dictionary definitions -- they only indicate sufficient conditions of the headword .
For example , the definition of tree in LDOCE tells nothing about leaves : tree n 1 a tall plant with a wooden trunk and branches , that lives for many years 2 a bush or other plant with a treelike form 3 a drawing with a branching form , esp .
as used for showing family relationships However , the definition is followed by pictures of leafy trees providing readers with connotational and extensional stereotypes of trees .
In the proposed method , the definitions in LDOCE are treated as word lists , though they are phrases with syntactic structures .
Let us consider the following definition of lift :
Anyone can imagine that something is moving upward .
But , such a movement can not be represented in the activated pattern produced from the phrase .
The meaning of a phrase , sentence , or text should be represented as pattern changing in time , though what we need is static and paradigmatic relation .
This paradox also arises in measuring the similarity between texts and the text coherence .
As we have seen in Section  , there is a difference between the similarity of texts and the similarity of word lists , and also between the coherence of a text and cohesiveness of a word list .
However , so far as the similarity between words is concerned , we assume that activated patterns on Paradigme will approximate the meaning of words , like a still picture can express a story .
We described measurement of semantic similarity between words .
The similarity between words is computed by spreading activation on the semantic network Paradigme which is systematically constructed from a subset of the English dictionary LDOCE .
Paradigme can directly compute the similarity between any two words in LDV , and indirectly the similarity of all the other words in LDOCE .
The similarity between words provides a new method for analysing the structure of text .
It can be applied to computing the similarity between texts , and measuring the cohesiveness of a text which suggests coherence of the text , as we have seen in Section  .
And , we are now applying it to text segmentation Grosz and Sidner 1986 , Youmans 1991 , i.e. to capture the shifts of coherent scenes in a story .
In future research , we intend to deal with syntagmatic relations between words .
Meaning of a text lies in the texture of paradigmatic and syntagmatic relations between words Hjelmslev 1943 .
Paradigme provides the former dimension -- an associative system of words -- as a screen onto which the meaning of a word is projected like a still picture .
The latter dimension -- syntactic process -- will be treated as a film projected dynamically onto Paradigme .
This enables us to measure the similarity between texts as a syntactic process , not as word lists .
We regard Paradigme as a field for the interaction between text and episodes in memory -- the interaction between what one is hearing or reading and what one knows Schank 1990 .
The meaning of words , sentences , or even texts can be projected in a uniform way on Paradigme , as we have seen in Section  and  .
Similarly , we can project text and episodes , and recall the most relevant episode for interpretation of the text .
The semantic network Paradigme is systematically constructed from the small and closed English dictionary Glossme .
Each entry of Glossme is mapped onto a node of Paradigme in the following way .
( See also Figure  and  . )
For each entry  in Glossme , map each unit  in  onto a subrfrant  of the corresponding node  in Paradigme .
Each word  is mapped onto a link or links in  , in the following way :
Let  be the reciprocal of the number of appearance of  ( as its root form ) in Glossme .
If  is in a head-part , let  be doubled .
Find nodes  corresponds to  ( ex. red  { red_1 , red_2 } ) .
Then , divide  into  in proportion to their frequency .
Thus ,  becomes a set of links :  , where  is a link with thickness  .
Then , normalize thickness of the links as  , in each  .
For each node  , compute thickness  of each subrfrant  in the following way :
Let  be  .
( Note that  = 2:1 . )
Normalize thickness  as  , in each  .
Generate rfr of each node in Paradigme , in the following way :
For each node  in Paradigme , let its rfr  be an empty set .
For each  , for each subrfrant  of  , for each link  in  :
Thus , each  becomes a set of links :  , where  is a link with thickness  .
Then , normalize thickness of the links as  , in each  .
Each node  of the semantic network Paradigme computes its activity value  at time  as follows :
where  and  are activity ( at time T ) collected from the nodes referred in the rfrant and rfr respectively ;  is activity given from outside ( at time T ) ; the output function  limits the value to [ 0,1 ] .
 is activity of the most plausible subrfrant in  , defined as follows :
where  is thickness of the j-th subrfrant of  .
 is the sum of weighted activity of the nodes referred in the j-th subrfrant of  , defined as follows :
where  is thickness of the k-th link of  , and  is activity ( at time T ) of the node referred by the k-th link of  .
 is weighted activity of the nodes referred in the rfr  of  :
where  is thickness of the k-th link of  , and  is activity ( at time T ) of the node referred by the k-th link of  .
