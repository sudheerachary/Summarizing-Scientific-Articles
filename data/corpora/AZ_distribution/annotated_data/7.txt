An extragrammatical sentence is what a normal parser fails to analyze .
It is important to recover it using only syntactic information although results of recovery are better if semantic factors are considered .
A general algorithm for least-errors recognition , which is based only on syntactic information , was proposed by G. Lyon to deal with the extragrammaticality .
We extended this algorithm to recover extragrammatical sentence into grammatical one in running text .
Our robust parser with recovery mechanism - extended general algorithm for least-errors recognition - can be easily scaled up and modified because it utilize only syntactic information .
To upgrade this robust parser we proposed heuristics through the analysis on the Penn treebank corpus .
The experimental result shows 68 % - 77 % accuracy in error recovery .
Extragrammatical sentences include patently ungrammatical constructions as well as utterances that may be grammatically acceptable but are beyond the syntactic coverage of a parser , and any other difficult ones that are encountered in parsing Carbonell and Hayes 1983 .
Above examples show that people are used to write same meaningful sentences differently .
In addition , people are prone to mistakes in writing sentences .
So , the bulk of written sentences are open to the extragrammaticality .
In the Penn treebank tree-tagged corpus Marcus 1991 , for instance , about 80 percents of the rules are concerned with peculiar sentences which include inversive , elliptic , parenthetic , or emphatic phrases .
For example , we can drive a rule VP  vb NP comma rb comma PP from the following sentence .
A robust parser is one that can analyze these extragrammatical sentences without failure .
However , if we try to preserve robustness by adding such rules whenever we encounter an extragrammatical sentence , the rulebase will grow up rapidly , and thus processing and maintaining the excessive number of rules will become inefficient and impractical .
Therefore , extragrammatical sentences should be handled by some recovery mechanism ( s ) rather than by a set of additional rules .
Many researchers have attempted several techniques to deal with extragrammatical sentences such as Augmented Transition Network ( ATN ) Kwasny and Sondheimer 1981 , network-based semantic grammar Hendrix 1977 , partial pattern matching Hayes and Mouradian 1981 , conceptual case frame Schank et al. 1980 , and multiple cooperating methods Hayes and Carbonell 1981 .
Above mentioned techniques take into account various semantic factors depending on specific domains on question in recovering extragrammatical sentences .
Whereas they can provide even better solutions intrinsically , they are usually ad-hoc and are lack of extensibility .
Therefore , it is important to recover extragrammatical sentences using syntactic factors only , which are independent of any particular system and any particular domain .
Mellish 1989 introduced some chart-based techniques using only syntactic information for extragrammatical sentences .
This technique has an advantage that there is no repeating work for the chart to prevent the parser from generating the same edge as the previously existed edge .
Also , because the recovery process runs when a normal parser terminates unsuccessfully , the performance of the normal parser does not decrease in case of handling grammatical sentences .
However , his experiment was not based on the errors in running texts but on artificial ones which were randomly generated by human .
Moreover , only one word error was considered though several word errors can occur simultaneously in the running text .
A general algorithm for least-errors recognition Lyon 1974 , proposed by G. Lyon , is to find out the least number of errors necessary to successful parsing and recover them .
Because this algorithm is also syntactically oriented and based on a chart , it has the same advantage as that of Mellish 's parser .
When the original parsing algorithm terminates unsuccessfully , the algorithm begins to assume errors of insertion , deletion and mutation of a word .
For any input , including grammatical and extragrammatical sentences , this algorithm can generate the resultant parse tree .
At the cost of the complete robustness , however , this algorithm degrades the efficiency of parsing , and generates many intermediate edges .
In this paper , we present a robust parser with a recovery mechanism .
We extend the general algorithm for least-errors recognition to adopt it as the recovery mechanism in our robust parser .
Because our robust parser handle extragrammatical sentences with this syntactic information oriented recovery mechanism , it can be independent of a particular system or particular domain .
Also , we present the heuristics to reduce the number of edges so that we can upgrade the performance of our parser .
This paper is organized as follows : We first review a general algorithm for least-errors recognition .
Then we present the extension of this algorithm , and the heuristics adopted by the robust parser .
Next , we describe the implementation of the system and the result of the experiment of parsing real sentences .
Finally , we make conclusion with future direction .
The general algorithm for least-errors recognition Lyon 1974 , which is based on Earley 's algorithm , assumes that sentences may have insertion , deletion , and mutation errors of terminal symbols .
The objective of this algorithm is to parse input string with the least number of errors .
A state used in this algorithm is quadruple  , where p is a production number in grammar , j marks a position in  , f is a start position of the state in input string , and e is an error value .
A final state  denotes recognition of a phrase  with e errors where  is a number of components in rule p .
A stateset  , where i is the position of the input , is an ordered set of states .
States within a stateset are ordered by ascending value of j , within a p within a f ; f takes descending value .
When adding to statesets , if state  is a candidate for admission to a stateset which already has a similar member  and e '  e , then  is rejected .
However , if  , then  is replaced by .
The algorithm works as follows : A procedure SCAN is carried out for each state in  .
SCAN checks various correspondences of input token  against terminal symbols in RHS of rules .
Once SCAN is done , COMPLETER substitutes all final states of  into all other analyses which can use them as components .
SCAN handles states of  , checking each input terminal against requirements of states in  and various error hypotheses .
Figure  shows how SCAN processes .
Let  be j-th component of  and  be i-th word of input string .
perfect match :
If  then add  to  if possible .
insertion-error hypothesis :
Add  to  if possible .
 is the cost of an insertion-error for a terminal symbol .
deletion-error hypothesis :
If  is terminal , then add  to  if possible .
 is the cost of a deletion-error for a terminal symbol .
mutation-error hypothesis :
If  is terminal but not equal to  , then add  to  if possible .
 is the cost of a mutation-error for a terminal symbol .
COMPLETER handles substitution of final states in  like that of original Earley 's algorithm .
Each final state means the recognition of a nonterminal .
The algorithm in section  can analyze any input string with the least number of errors .
But this algorithm can handle only the errors of terminal symbols because it doesn't consider the errors of nonterminal nodes .
In the real text , however , the insertion , deletion , or inversion of a phrase - namely , nonterminal node - occurs more frequently .
So , we extend the original algorithm in order to handle the errors of nonterminal symbols as well .
In our extended algorithm , the same SCAN as that of the original algorithm is used , while COMPLETER is modified and extended .
Figure  shows the processing of extended-COMPLETER .
In figure  , [ NP ] denotes the final state whose rule has NP as its LHS .
In other words , it means the recognition of a noun phrase .
If there is a final state  in  ,
phrase perfect match
If there exists a state  in  and  then add  into  .
phrase insertion-error hypothesis
If there exists a state  in  then add  into  if possible .
 is the cost of a insertion-error for a nonterminal symbol .
phrase deletion-error hypothesis
If there exists a state  in  and  is a nonterminal then add  into  if possible .
 is the cost of a deletion-error for a nonterminal symbol .
phrase mutation-error hypothesis
If there exists a state  in  and  is a nonterminal but not equal to  then add  into  if possible .
 is the cost of a mutation-error for a nonterminal symbol .
The extended least-errors recognition algorithm can handle not only terminal errors but also nonterminal errors .
The robust parser using the extended least-errors recognition algorithm overgenerates many error-hypothesis edges during parsing process .
To cope with this problem , we adjust error values according to the following heuristics .
Edges with more error values are regarded as less important ones , so that those edges are processed later than those of less error values .
Heuristics 1 : error types
The analysis on 3,538 sentences of the Penn treebank corpus WSJ shows that there are 498 sentences with phrase deletions and 224 sentences with phrase insertions .
So , we assign less error value to the deletion-error hypothesis edge than to the insertion - and mutation-errors .
where  is the error cost of a terminal symbol ,  is the error cost of a nonterminal symbol .
Heuristics 2 : fiducial nonterminal
People often make mistakes in writing English .
These mistakes usually take place rather between small constituents such as a verbal phrase , an adverbial phrase and noun phrase than within small constituents themselves .
The possibility of error occurrence within noun phrases are lower than between a noun phrase and a verbal phrase , a preposition phrase , an adverbial phrase .
So , we assume some phrases , for example noun phrases , as fiducial nonterminals , which means error-free nonterminals .
When handling sentences , the robust parser assings more error values (  ) to the error hypothesis edge occurring within a fiducial nonterminal .
Heuristics 3 : kinds of terminal symbols
Some terminal symbols like punctuation symbols , conjunctions and particles are often misused .
So , the robust parser assigns less error values (  ) to the error hypothesis edges with these symbols than to the other terminal symbols .
Heuristics 4 : inserted phrases between commas or parentheses
Most of inserted phrases are surrounded by commas or parentheses .
For example ,
We will assign less error values (  ) to the insertion-error hypothesis edges of nonterminals which are embraced by comma or parenthesis .
 and  are weights for the error of terminal nodes , and  is a weight for the error of nonterminal nodes .
The error value e of an edge is calculated as follows .
All error values are additive .
The error value e for a rule  , where a is a terminal node and A is a nonterminal node , is
where  ,  and  is an error value of a child edge .
By these heuristics , our robust parser can process only plausible edges first , instead of processing all generated edges at the same time , so that we can enhance the performance of the robust parser and result in the great reduction in the number of resultant trees .
Our robust parsing system is composed of two modules .
One module is a normal parser which is the bottom-up chart parser .
The other is a robust parser with the error recovery mechanism proposed herein .
At first , an input sentence is processed by the normal parser .
If the sentence is within the grammatical coverage of the system , the normal parser succeed to analyze it .
Otherwise , the normal parser fails , and then the robust parser starts to execute with edges generated by the normal parser .
The result of the robust parser is the parse trees which are within the grammatical coverage of the system .
The overview of the system is shown in figure  .
To show usefulness of the robust parser proposed in this paper , we made some experiments .
Rule
We can derive 4,958 rules and their frequencies out of 14,137 sentences in the Penn treebank tree-tagged corpus , the Wall Street Journal .
The average frequency of each rule is 48 times in the corpus .
Of these rules , we remove rules which occurs fewer times than the average frequency in the corpus , and then only 192 rules are left .
These removed rules are almost for peculiar sentences and the left rules are very general rules .
We can show that our robust parser can compensate for lack of rules using only 192 rules with the recovery mechanism and heuristics .
Test set
First , 1,000 sentences are selected randomly from the WSJ corpus , which we have referred to in proposing the robust parser .
Of these sentences , 410 are failed in normal parsing , and are processed again by the robust parser .
To show the validity of these heuristics , we compare the result of the robust parser using heuristics with one not using heuristics .
Second , to show the adaptability of our robust parser ,
same experiments are carried out on 1,000 sentences from the ATIS corpus in Penn treebank , which we haven't referred to when we propose the robust parser .
Among 1,000 sentences from the ATIS , 465 sentences are processed by the robust parser after the failure of the normal parsing .
Parameter adjustment
We chose the best parameters of heuristics by executing several experiments .
Accuracy is measured as the percentage of constituents in the test sentences which do not cross any Penn treebank constituents Black 1991 .
Table  shows the results of the robust parser on WSJ .
In table  , 5th , 6th and 7th raw mean that the percentage of sentences which have no crossing constituents , less than one crossing and less than two crossing respectively .
With heuristics , our robust parser can enhance the processing time and reduce the number of edges .
Also , the accuracy is improved from 72.8 % to 77.1 % even if the heuristics differentiate edges and prefer some edges .
It shows that the proposed heuristics is valid in parsing the real sentences .
The experiment says that our robust parser with heuristics can recover perfectly about 23 sentences out of 100 sentences which are just failed in normal parsing , as the percentage of no-crossing sentences is about 23.28 .
Table  is the results of the robust parser on ATIS which we did not refer to before .
The accuracy of the result on ATIS is lower than WSJ because the parameters of the heuristics are adjusted not by ATIS itself but by WSJ .
However , the percentage of sentences with constituents crossing less than 2 is higher than the WSJ , as sentences of ATIS are more or less simple .
The experimental results of our robust parser show high accuracy in recovery even though 96 % of total rules are removed .
It is impossible to construct complete grammar rules in the real parsing system to succeed in analyzing every real sentence .
So , parsing systems are likely to have extragrammatical sentences which cannot be analyzed by the systems .
Our robust parser can recover these extragrammatical sentences with 68 - 77 % accuracy .
It is very interesting that parameters of heuristics reflect the characteristics of the test corpus .
For example , if people tend to write sentences with inserted phrases , then the parameter  must increase .
Therefore we can get better results if the parameter are fitted to the characteristics of the corpus .
In this paper , we have presented the robust parser with the extended least-errors recognition algorithm as the recovery mechanism .
This robust parser can easily be scaled up and applied to various domains because this parser depends only on syntactic factors .
To enhance the performance of the robust parser for extragrammatical sentences , we proposed several heuristics .
The heuristics assign the error values to each error-hypothesis edge , and edges which has less error values are processed first .
So , not all the generated edges are processed by the robust parser , but the most plausible parse trees can be generated first .
The accuracy of the recovery in our robust parser is about 68 % - 77 % .
Hence , this parser is suitable for systems in real application areas .
Our short term goal is to propose an automatic method that can learn parameter values of heuristics by analyzing the corpus .
We expect that automatically learned values of parameters can upgrade the performance of the parser .
This work was supported ( in part ) by Korea Science and Engineering Foundation ( KOSEF ) through Center for Artificial Intelligence Research ( CAIR ) , the Engineering Research Center ( ERC ) of Excellence Program .
